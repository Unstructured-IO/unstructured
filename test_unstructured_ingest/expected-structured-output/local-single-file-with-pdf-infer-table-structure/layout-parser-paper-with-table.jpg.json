[
  {
    "type": "Title",
    "element_id": "8e5839c2fb9b4d6b78cd2b1c1f5bed02",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "LayoutParser: A Unified Toolkit for DL-Based DIA 5"
  },
  {
    "type": "FigureCaption",
    "element_id": "f2c0641f368a9449a58ec35931e4ae81",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "Table 1: Current layout detection models in the LayoutParser model zoo"
  },
  {
    "type": "Table",
    "element_id": "993e57879fdf1ca00d017561e6d275f4",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "Dataset | Base Model'| Large Model | Notes PubLayNet [38] F/M M Layouts of modern scientific documents PRImA [3] M - Layouts of scanned modern magazines and scientific reports Newspaper [17] P - Layouts of scanned US newspapers from the 20th century TableBank [15) P P Table region on modern scientific and business document HlDataset (31) | P/ M - Layouts of history Japanese documents"
  },
  {
    "type": "UncategorizedText",
    "element_id": "8a8de823d5ed3e12746a62ef169bcf37",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "?"
  },
  {
    "type": "FigureCaption",
    "element_id": "d9aa29d129c76a1ddf8230bfa3265a7b",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "For each dataset, we train several models of different sizes for different needs (the trade-off between accuracy vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101 backbones [13, respectively. One can train models of diffecent architectures, like Faster R-CNN (28] (F) and Mask R-CNN [12] (M). For example, an F in the Large Model column indicates it has & Faster R-CNN model trainad using the ResNet 101 backbone. The platform is maintained and a number of additicas will be made to the model 200 in coming months."
  },
  {
    "type": "NarrativeText",
    "element_id": "33dffbb2a495c5e5f9d2677ce3ec87c1",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "layout data structures, which are optimized for efficiency and versatility. 3) When necessary, users can employ existing or customized OCR models via the unified API provided in the OCR module. 4) LayoutParser comes with a set of utility functions for the visualization and storage of the layout data. 5) LayoutParser is also highly customizable, via its integration with functions for layout data annotation and model training. We now provide detailed descriptions for each component."
  },
  {
    "type": "Title",
    "element_id": "958174bfb8153f0b2c1d247196bcf8b1",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "3.1 Layout Detection Models"
  },
  {
    "type": "NarrativeText",
    "element_id": "6308564d51ecc38117d56ca4d886e7b7",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "In LayoutParser, a layout model takes a document image as an input and generates a list of rectangular boxes for the target content regions. Different from traditional methods, it relies on deep convolutional neural networks rather than manually curated rules to identify content regions. It is formulated as an object detection problem and state-of-the-art models like Faster R-CNN [28] and Mask R-CNN [12] are used. This yields prediction results of high accuracy and makes it possible to build a concise, generalized interface for layout detection. LayoutParser, built upon Detectron2 [35], provides a minimal API that can perform layout detection with only four lines of code in Python:"
  },
  {
    "type": "ListItem",
    "element_id": "d4e57733a52ccc54d1b78e8d112612be",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "e import layoutparser as lp"
  },
  {
    "type": "Title",
    "element_id": "54e4b069a28c5162adcb2f6b2293e528",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "e Lo"
  },
  {
    "type": "ListItem",
    "element_id": "cbb879049df7bd737ffc487a61b05f70",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "image = cv2.imread(\"image_file\") # load images"
  },
  {
    "type": "ListItem",
    "element_id": "3c820b1130e318d98c2bf71356bca106",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "model = lp.Detectron2LayoutModel ("
  },
  {
    "type": "ListItem",
    "element_id": "caca9c1d6a0f23deaf877d4e7260b602",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "\"lp://PublLayNet/faster_rcnn R_60_FPN_3x/config\")"
  },
  {
    "type": "ListItem",
    "element_id": "4e593a007fa94e6b5be36fec2be86b7b",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "layout = model.detect(image)"
  },
  {
    "type": "NarrativeText",
    "element_id": "06a904ffc64759ca77681d832b886dff",
    "metadata": {
      "data_source": {},
      "filetype": "image/jpeg",
      "page_number": 1
    },
    "text": "LayoutParser provides a wealth of pre-trained model weights uging various datasets covering different languages, time periods, and document types. Due to domain shift [7], the prediction performance can notably drop when models are ap- plied to target samples that are significantly different from the training dataset. As document structures and layouts vary greatly in different domains, it is important to select models trained on a dataset similar to the test samples. A semantic syntax is used for initializing the model weights in LayoutParser, using both the dataset name and model name 1p://<dataset-name>/<model-architecture-name>."
  }
]