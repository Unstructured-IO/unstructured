name: Partition Benchmark

# Runs on every PR targeting main to detect regressions.
# Can also be triggered manually to establish or inspect a new baseline.
on:
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read

env:
  NLTK_DATA: ${{ github.workspace }}/nltk_data
  PYTHON_VERSION: "3.12"
  # Number of timed iterations per file.
  # Kept at 1 in CI to minimise runner time; raise locally for more stable averages.
  NUM_ITERATIONS: "1"
  # Fraction by which the current total may exceed the stored best before the
  # job is marked as failed. 0.20 = 20%.
  REGRESSION_THRESHOLD: "0.20"
  # Increment to bust all stored caches (e.g. after a deliberate performance
  # trade-off is accepted).
  CACHE_VERSION: "v1"
  # S3 location for metrics – matches core-product convention.
  S3_METRICS_BUCKET_KEY: utic-metrics/ci-metrics
  S3_BENCHMARK_PATH: open-source/partition-benchmark/benchmark_best.json

jobs:
  setup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/base-cache
        with:
          python-version: ${{ env.PYTHON_VERSION }}

  benchmark:
    name: Measure and compare partition() runtime
    runs-on: ubuntu-latest
    needs: [setup]

    steps:
      # ------------------------------------------------------------------ #
      # 1. Source & Python environment                                      #
      # ------------------------------------------------------------------ #
      - uses: actions/checkout@v4

      - uses: ./.github/actions/base-cache
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libmagic-dev poppler-utils libreoffice
          sudo add-apt-repository -y ppa:alex-p/tesseract-ocr5
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-kor

      # ------------------------------------------------------------------ #
      # 2. Cache HuggingFace models                                        #
      #    hi_res downloads layout-detection models on first use.          #
      #    Caching keeps benchmark timings stable across runs.             #
      # ------------------------------------------------------------------ #
      - name: Restore HuggingFace model cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-${{ github.sha }}
          restore-keys: |
            hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-
            hf-models-${{ runner.os }}-

      # ------------------------------------------------------------------ #
      # 3. Run the benchmark                                               #
      #    Writes per-file timings + __total__ to benchmark_results.json.  #
      # ------------------------------------------------------------------ #
      - name: Run partition benchmark
        env:
          NUM_ITERATIONS: ${{ env.NUM_ITERATIONS }}
        run: |
          uv run --no-sync python scripts/performance/benchmark_partition.py \
            benchmark_results.json

      - name: Save HuggingFace model cache
        uses: actions/cache/save@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-${{ github.sha }}

      # ------------------------------------------------------------------ #
      # 4. Download the stored best runtime from S3                        #
      #    continue-on-error: first run will have nothing stored yet.      #
      # ------------------------------------------------------------------ #
      - name: Download previous best from S3
        continue-on-error: true
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_EVAL_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_EVAL_SECRET_KEY }}
        run: |
          aws s3 cp \
            "s3://${{ env.S3_METRICS_BUCKET_KEY }}/${{ env.S3_BENCHMARK_PATH }}" \
            benchmark_best.json

      # ------------------------------------------------------------------ #
      # 5. Compare results; fail on regression                             #
      #    compare_benchmark.py:                                           #
      #      - Logs a per-file table with deltas.                          #
      #      - Exits 1 if current > best * (1 + threshold).               #
      #      - Overwrites benchmark_best.json when current is faster.      #
      # ------------------------------------------------------------------ #
      - name: Compare results against stored best
        id: compare
        run: |
          uv run --no-sync python scripts/performance/compare_benchmark.py \
            benchmark_results.json \
            benchmark_best.json \
            ${{ env.REGRESSION_THRESHOLD }}

      # ------------------------------------------------------------------ #
      # 6. Upload the (possibly updated) best result back to S3            #
      #    compare_benchmark.py only overwrites benchmark_best.json when   #
      #    the current run is strictly faster, so the true minimum is      #
      #    preserved.                                                      #
      # ------------------------------------------------------------------ #
      - name: Upload best result to S3
        continue-on-error: true
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.S3_EVAL_ACCESS_KEY }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.S3_EVAL_SECRET_KEY }}
        run: |
          aws s3 cp \
            benchmark_best.json \
            "s3://${{ env.S3_METRICS_BUCKET_KEY }}/${{ env.S3_BENCHMARK_PATH }}"

      # ------------------------------------------------------------------ #
      # 7. Upload artifacts – always, so regressions can be inspected      #
      # ------------------------------------------------------------------ #
      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            benchmark_results.json
            benchmark_best.json
          retention-days: 30
