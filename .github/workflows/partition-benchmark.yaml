name: Partition Benchmark

# Runs on every PR targeting main.
# Can also be triggered manually to establish or inspect a new baseline.
on:
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: read

env:
  NLTK_DATA: ${{ github.workspace }}/nltk_data
  PYTHON_VERSION: "3.12"
  # Number of timed iterations per file, matching NUM_ITERATIONS in benchmark-local.sh.
  # Kept at 1 in CI to minimise runner time; raise locally for more stable averages.
  NUM_ITERATIONS: "1"
  # Fraction by which the current total may exceed the stored best before the
  # job is marked failed.  0.20 = 20%.
  REGRESSION_THRESHOLD: "0.20"
  # Increment to bust all stored best-runtime caches (e.g. after a deliberate
  # performance trade-off is accepted).
  CACHE_VERSION: "v1"
  RESULTS_DIR: "scripts/performance/partition-speed-test"

jobs:
  setup:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: ./.github/actions/base-cache
        with:
          python-version: ${{ env.PYTHON_VERSION }}

  benchmark:
    name: Measure and compare partition() runtime
    runs-on: ubuntu-latest
    needs: [setup]

    steps:
      # ------------------------------------------------------------------ #
      # 1. Source & Python environment                                       #
      # ------------------------------------------------------------------ #
      - uses: actions/checkout@v4

      - uses: ./.github/actions/base-cache
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y libmagic-dev poppler-utils libreoffice
          sudo add-apt-repository -y ppa:alex-p/tesseract-ocr5
          sudo apt-get update
          sudo apt-get install -y tesseract-ocr tesseract-ocr-kor

      # ------------------------------------------------------------------ #
      # 2. Cache HuggingFace models                                         #
      #    hi_res downloads layout-detection models on first use.           #
      #    Caching keeps benchmark timings stable across runs.              #
      # ------------------------------------------------------------------ #
      - name: Restore HuggingFace model cache
        uses: actions/cache/restore@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-${{ github.sha }}
          restore-keys: |
            hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-
            hf-models-${{ runner.os }}-

      # ------------------------------------------------------------------ #
      # 3. Run the benchmark                                                #
      #    No output path argument needed - script defaults to             #
      #    scripts/performance/partition-speed-test/benchmark_results.json  #
      # ------------------------------------------------------------------ #
      - name: Run partition benchmark
        env:
          NUM_ITERATIONS: ${{ env.NUM_ITERATIONS }}
        run: |
          uv run --no-sync python scripts/performance/benchmark_partition.py

      - name: Save HuggingFace model cache
        uses: actions/cache/save@v4
        with:
          path: ~/.cache/huggingface
          key: hf-models-${{ runner.os }}-${{ env.CACHE_VERSION }}-${{ github.sha }}

      # ------------------------------------------------------------------ #
      # 4. Restore the stored best runtime                                  #
      #    Each PR compares against the most recent cached best.            #
      # ------------------------------------------------------------------ #
      - name: Restore best benchmark result
        uses: actions/cache/restore@v4
        with:
          path: ${{ env.RESULTS_DIR }}/benchmark_best.json
          key: benchmark-best-${{ env.CACHE_VERSION }}-${{ github.sha }}
          restore-keys: |
            benchmark-best-${{ env.CACHE_VERSION }}-

      # ------------------------------------------------------------------ #
      # 5. Compare results; fail on regression                              #
      # ------------------------------------------------------------------ #
      - name: Compare results against stored best
        id: compare
        run: |
          uv run --no-sync python scripts/performance/compare_benchmark.py \
            ${{ env.RESULTS_DIR }}/benchmark_results.json \
            ${{ env.RESULTS_DIR }}/benchmark_best.json \
            ${{ env.REGRESSION_THRESHOLD }}

      - name: Save best benchmark result
        uses: actions/cache/save@v4
        with:
          path: ${{ env.RESULTS_DIR }}/benchmark_best.json
          key: benchmark-best-${{ env.CACHE_VERSION }}-${{ github.sha }}

      # ------------------------------------------------------------------ #
      # 6. Upload artifacts - always, so regressions can be inspected      #
      # ------------------------------------------------------------------ #
      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: ${{ env.RESULTS_DIR }}/
          retention-days: 30
