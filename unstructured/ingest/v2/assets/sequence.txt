title Ingest Flow


Pipeline->Index: Pipeline.indexer_step.run()
Index->Data Provider:fetch list of docs with metadata
Data Provider->Index:
Index->Local Filesystem:for each record, save the metadata as a json file
Index->Pipeline: pipeline records a list of files
Pipeline->Download: Pipeline.downloader_step(records)
Download->Local Filesystem: Fetch the associated metadata
Local Filesystem->Download:
Download->Data Provider: Get raw data from data provider
Download->Local Filesystem: Persist the data as raw files
Download->Pipeline: Send back a reference to the local file to process
Pipeline-->Uncompress: Optionally run if flag set to True
Uncompress->Local Filesystem: Extract tar and zip files
Uncompress->Local Filesystem: New metadata records are created for new extracted files
Uncompress->Pipeline: Send back list of pointers to new metadata files
Pipeline->Partition: Pipeline.partitioner_step(downloaded_data)
Partition-->Unstructured Api: If credentials passed in,\npassed file data to API for partitioning
Unstructured Api->Partition:
Partition->Local Filesystem: Persist results
Partition->Pipeline: Pointers to persisted results
Pipeline-->Chunk:  Optionally Pipeline.chunker_step.run(records)
Chunk-->Unstructured Api: If credentials passed in,\npassed file data to API for chunking
Unstructured Api->Chunk:
Chunk->Local Filesystem: Persist results
Chunk->Pipeline: Pointers to persisted results
Pipeline-->Embed: Optionally Pipeline.embed_step.run(records)
Embed-->Embedder Api: Depending on which embedder\nis chosen, make API calls to provider
Embed->Local Filesystem: Persist results
Embed->Pipeline: Pointers to persisted results
Pipeline->Stage: Optionally Pipeline.stager_step.run(records)
Stage->Local Filesystem: manipulate the records to better upload
Stage->Pipeline: Pointers to persisted results
Pipeline->Upload: Pipeline.upload_step.run()
Upload->Data Destination:
Pipeline->Local Filesystem: Cleanup
